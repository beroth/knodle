{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_fn = \"/home/yoga/workspace/knodle_hackathon_fork/knodle/hackathon_seq/atis.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(atis_fn, \"rb\") as input_file:\n",
    "    atis_dataset = json.load(input_file, encoding=\"UTF8\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(atis_fn, \"rb\") as input_file:\n",
    "#    atis_dataset = pickle.load(input_file, encoding=\"UTF8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = atis_dataset['train_sents'][0]\n",
    "#word_to_id = atis_dataset['vocab']\n",
    "#id_to_word = {word_to_id[word]:word for word in word_to_id}\n",
    "#id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id_to_word[wid] for wid in s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = atis_dataset['train_labels'][0]\n",
    "#label_to_id = atis_dataset['label_dict']\n",
    "#id_to_label = {label_to_id[label]:label for label in label_to_id}\n",
    "#id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(id_to_word[wid], id_to_label[lid]) for (wid, lid) in zip(s1,l1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knodle/transformation/majority.py:\n",
    "#    if not use_probabilistic_labels:\n",
    "#        # convert labels represented as a prob distribution to a single label using majority voting\n",
    "#        kwargs = {\"choose_random_label\": True, \"other_class_id\": other_class_id}\n",
    "#        noisy_y_train = np.apply_along_axis(probabilies_to_majority_vote, axis=1, arr=noisy_y_train, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = atis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_dev_sents = data[\"train_sents\"] # list of lists\n",
    "train_dev_labels = data[\"train_labels\"] # list of lists\n",
    "num_train = math.floor(0.8 * len(train_dev_sents))\n",
    "train_sents = train_dev_sents[:num_train]\n",
    "train_labels = train_dev_labels[:num_train]\n",
    "dev_sents = train_dev_sents[num_train:]\n",
    "dev_labels = train_dev_labels[num_train:]\n",
    "test_sents = data[\"test_sents\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "word_to_id = data[\"vocab\"]\n",
    "label_to_id = data[\"label_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "VOCAB_SIZE = len(word_to_id)\n",
    "NUM_LABELS = len(label_to_id)\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE=50\n",
    "MAX_LENGTH=20\n",
    "KERNEL_SIZE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We reorganize the data so that \"<PAD>\" has id 0, \"<UNK>\" has id 1, and the label 'O' has id 0 (and all words are still represented). Make sure to also change word_to_id and label_to_id.\n",
    "id_to_word = {word_to_id[word]: word for word in word_to_id}\n",
    "id_to_label = {label_to_id[label]: label for label in label_to_id}\n",
    "\n",
    "def switch_idx(sents, i, j):\n",
    "    D = {i:j, j:i}\n",
    "    return [[D.get(word, word) for word in sent] for sent in sents]\n",
    "\n",
    "for i, j in [(word_to_id[\"<PAD>\"], 0), (word_to_id[\"<UNK>\"], 1)]:\n",
    "    train_sents = switch_idx(train_sents, i, j)\n",
    "    dev_sents = switch_idx(dev_sents, i, j)\n",
    "    test_sents = switch_idx(test_sents, i, j)\n",
    "    \n",
    "train_labels = switch_idx(train_labels, label_to_id[\"O\"], 0)\n",
    "dev_labels = switch_idx(dev_labels, label_to_id[\"O\"], 0)\n",
    "test_labels = switch_idx(test_labels, label_to_id[\"O\"], 0)\n",
    "\n",
    "word_to_id[id_to_word[0]] = word_to_id[\"<PAD>\"]\n",
    "word_to_id[id_to_word[1]] = word_to_id[\"<UNK>\"]\n",
    "label_to_id[id_to_label[0]] = label_to_id[\"O\"]\n",
    "\n",
    "label_to_id[\"O\"] = 0\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<UNK>\"] = 1\n",
    "\n",
    "id_to_word = {word_to_id[word]: word for word in word_to_id}\n",
    "id_to_label = {label_to_id[label]: label for label in label_to_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def do_padding(sequences, length = MAX_LENGTH):\n",
    "    return pad_sequences(sequences, maxlen = length)\n",
    "\n",
    "train_sents_padded = do_padding(train_sents)\n",
    "dev_sents_padded = do_padding(dev_sents)\n",
    "test_sents_padded = do_padding(test_sents)\n",
    "\n",
    "train_labels_padded = torch.LongTensor(do_padding(train_labels))\n",
    "dev_labels_padded = do_padding(dev_labels)\n",
    "test_labels_padded = do_padding(test_labels)\n",
    "\n",
    "# TODO: if we want 1-hot vectors:\n",
    "#train_labels_padded = to_categorical(do_padding(train_labels), NUM_LABELS)\n",
    "#dev_labels_padded = to_categorical(do_padding(dev_labels), NUM_LABELS)\n",
    "#test_labels_padded = to_categorical(do_padding(test_labels), NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling/blob/master/dynamic_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer, with 0 vectors for the pads\n",
    "embeds = nn.Embedding(len(word_to_id), 50, padding_idx=0)\n",
    "\n",
    "# Create an LSTM layer\n",
    "lstm = nn.LSTM(50, 50, bidirectional=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = torch.LongTensor(train_sents_padded)\n",
    "embeddings = embeds(seqs)\n",
    "out_static, _ = lstm(embeddings)\n",
    "assert out_static.size(1) == embeddings.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_lens=...\n",
    "#packed_seqs = pack_padded_sequence(embeddings, seq_lens.tolist(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling/blob/master/train.py\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, padding_idx=0):\n",
    "        super(LstmModel, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeds = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False, batch_first=True)\n",
    "        self.emission = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, token_idxs):\n",
    "        embeds = self.embeds(token_idxs)\n",
    "        hidden, _ = self.lstm(embeds)\n",
    "        pred = self.emission(hidden)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmModel(vocab_size = len(word_to_id), tagset_size = len(label_to_id), embedding_dim=30, hidden_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_batch = torch.LongTensor(train_sents_padded[0:2])\n",
    "s_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(s_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.data_size = len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, new_lr):\n",
    "    \"\"\"\n",
    "    Shrinks learning rate by a specified factor.\n",
    "    :param optimizer: optimizer whose learning rates must be decayed\n",
    "    :param new_lr: new learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "start_epoch = 0  # start at this epoch\n",
    "batch_size = 10  # batch size\n",
    "lr = 0.015  # learning rate\n",
    "lr_decay = 0.05  # decay learning rate by this amount\n",
    "momentum = 0.9  # momentum\n",
    "workers = 1  # number of workers for loading data in the DataLoader\n",
    "epochs = 2  # number of epochs to run without early-stopping\n",
    "grad_clip = 5.  # clip gradients at this value\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clip gradients computed during backpropagation to prevent gradient explosion.\n",
    "    :param optimizer: optimized with the gradients to be clipped\n",
    "    :param grad_clip: gradient clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(SeqDataset(train_sents_padded, train_labels_padded), batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=workers, pin_memory=False)\n",
    "val_loader = torch.utils.data.DataLoader(SeqDataset(dev_sents_padded, dev_labels_padded), batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=workers, pin_memory=False)\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    for i, (tokens, labels) in enumerate(train_loader):\n",
    "    #for (tokens, labels) in train_loader:\n",
    "        \n",
    "#        print(tokens.shape)\n",
    "#        print(s_batch.shape)\n",
    "#        print(train_sents_padded.shape)\n",
    "        \n",
    "        #logits = model(s_batch)\n",
    "        #print(logits.shape)\n",
    "\n",
    "        # batchsize x nb_classes x seq_len\n",
    "        logits = model(tokens).permute(0,2,1)\n",
    "        #print(logits.shape)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        #labels = torch.LongTensor(labels)\n",
    "        \n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "                # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # One epoch's validation\n",
    "    #val_f1 = validate(val_loader=val_loader,\n",
    "    #                  model=model,\n",
    "    #                  crf_criterion=crf_criterion,\n",
    "    #                  vb_decoder=vb_decoder)\n",
    "    adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_to_newlabel = dict()\n",
    "\n",
    "for oldlabel in label_to_id.keys():\n",
    "    #print(label)\n",
    "    #print(label.split(\".\")[0].split(\"-\")[-1])\n",
    "    newlabel = oldlabel.split(\".\")[0].split(\"-\")[-1]\n",
    "    lf_to_newlabel[oldlabel] = newlabel\n",
    "\n",
    "del lf_to_newlabel[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_to_newlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(lf_to_newlabel.keys())))\n",
    "print(len(set(lf_to_newlabel.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. use original labels, without \"O\" to represent Z-matrix\n",
    "# 2. make T matrix\n",
    "# 3. do majority vote\n",
    "# 4. make new label vector\n",
    "# 5. train\n",
    "# 6. check result of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. use original labels, without \"O\" to represent Z-matrix\n",
    "train_labels_padded = to_categorical(do_padding(train_labels), NUM_LABELS)[:,:,1:]\n",
    "dev_labels_padded = to_categorical(do_padding(dev_labels), NUM_LABELS)[:,:,1:]\n",
    "test_labels_padded = to_categorical(do_padding(test_labels), NUM_LABELS)[:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(id_to_label.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlabel_to_id = dict()\n",
    "\n",
    "for i, nl in enumerate(set(lf_to_newlabel.values())):\n",
    "    newlabel_to_id[nl] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfid_to_nlid = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lf_id in id_to_label:\n",
    "    if lf_id == 0:\n",
    "        continue\n",
    "    lf_name = id_to_label[lf_id]\n",
    "    newlabel_name = lf_to_newlabel[lf_name]\n",
    "    nl_id = newlabel_to_id[newlabel_name]\n",
    "    # \"O\" is not a lf in the 1-hot encoding.\n",
    "    lfid_to_nlid[lf_id - 1] = nl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfid_to_nlid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t_matrix = np.zeros((len(lfid_to_nlid), len(newlabel_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lfid in lfid_to_nlid:\n",
    "    t_matrix[lfid, lfid_to_nlid[lfid]] = 1\n",
    "    \n",
    "t_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels_padded.shape)\n",
    "print(t_matrix.shape)\n",
    "# train_labels_padded <-> \"Z matrix\" <-> lf matches <-> original fine-grained 'labels'\n",
    "print(train_labels_padded.dot(t_matrix).shape)\n",
    "train_newlabels_padded = train_labels_padded.dot(t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_newlabels_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_label = train_newlabels_padded.max(axis=2)\n",
    "has_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = (train_newlabels_padded.argmax(axis=2) + 1) * has_label - 1\n",
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_newlabel = {i:l for l,i in newlabel_to_id.items()}\n",
    "\n",
    "id_to_newlabel[-1] = \"O\"\n",
    "id_to_newlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl1 = label_ids[0]\n",
    "#[(id_to_word[wid], id_to_label[lid]) for (wid, lid) in zip(s1,nl1)]\n",
    "nl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id_to_newlabel[lid] for lid in nl1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
